---
title: "数据分析理论与Python实战 📊"
subtitle: "第五章 数据分析与知识发现——一些常用的方法 💡"
---

## 本章概述 🗺️

本章将带你探索数据分析与知识发现的奇妙世界，我们将学习几种常用的方法，就像解锁一个个宝箱 🔑，发现数据中隐藏的秘密。主要包括：

- 📦 **分类分析 (Classification Analysis)**：将数据分门别类，就像整理房间一样。
- 🔗 **关联分析 (Association Analysis)**：寻找数据之间的关联，就像侦探 🕵️‍♀️ 发现线索。
- 🤝 **聚类分析 (Cluster Analysis)**：将相似的数据聚集在一起，就像朋友们聚会 🥳。
- 📈 **回归分析 (Regression Analysis)**：预测未来的趋势，就像天气预报 🌤️。

这些方法在金融、商业、科学研究等领域都有广泛的应用。通过学习这些方法，我们可以从数据中提取有价值的信息，并将其转化为知识，为决策提供支持。

## 什么是数据分析？ 🤔

::: {.callout-note}
数据分析是一个**迭代探索**的过程。它利用**统计学、机器学习**等方法，对数据进行**收集、清洗、转换、建模和可视化**，从而发现数据中的**模式、趋势、异常和关联**，并对这些发现进行**解释和评估**。最终目的是从数据中提取有价值的信息，将其转化为知识，为决策提供支持。

数据分析就像一位厨师 👨‍🍳，将原始食材（数据）经过一系列处理，变成美味佳肴（知识）。
:::

## 数据分析与Python 🐍

Python 是一种广泛应用于数据分析的编程语言，它就像一把瑞士军刀 🛠️，功能强大，方便实用。它具有以下优点：

- **易于学习和使用**: Python 语法简洁清晰，易于上手，就像搭乐高积木一样 🧱。
- **丰富的库和工具**: Python 拥有大量用于数据分析的库和工具，如 NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn 等，可以大大提高数据分析的效率，就像拥有了各种神奇的工具 🧰。
- **强大的社区支持**: Python 拥有庞大的用户社区，可以提供丰富的学习资源和技术支持，就像有一个智囊团 🧠 在你身边。
- **跨平台性**: Python 可以在各种操作系统上运行，包括 Windows, macOS 和 Linux，就像一名旅行家 🌍，可以在不同的地方工作。

::: {.callout-tip}
### Python小例子

```python
import pandas as pd

# 创建一个 DataFrame (数据表格) 就像制作一个 Excel 表格 📊
data = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],
        'Age': [25, 32, 18, 47],
        'City': ['New York', 'Paris', 'London', 'Tokyo']}
df = pd.DataFrame(data)

# 打印 DataFrame
print(df)

# 计算平均年龄
average_age = df['Age'].mean()
print(f"平均年龄: {average_age}")
```

这段代码演示了如何使用 Pandas 库创建一个简单的数据表格并计算平均年龄，就像用 Excel 做简单的计算一样。
:::

## 分类分析 🎯

::: {.callout-note}
分类是找出数据库中一组数据对象的共同特点并按照分类模式将其划分为不同的类，其目的是通过分类模型，将数据库中的数据项映射到某个给定的类别。

分类就像图书管理员 📚 将书籍按照主题分类，或者像垃圾分类 ♻️ 一样，将不同的物品放入不同的垃圾桶。
:::

### 监督学习 👀

分类学习是一类**监督学习 (Supervised Learning)** 的问题。

- **监督学习**: 从有标签的训练数据中学习模型，然后对未知数据进行预测。
    -   **训练数据**: 包含特征和标签的数据集，就像一本带有答案的练习册 📖。
    -   **标签**: 数据的类别或目标值，就像练习册中的答案 ✅。
- 常见的监督学习任务：
  -   **分类 (Classification)**: 预测离散的类别标签，就像判断一张图片是猫 🐱 还是狗 🐶。
  -   **回归 (Regression)**: 预测连续的目标值，就像预测明天的气温 🌡️。

### 分类问题的类型 🗂️

根据分类结果可以分为：

- **二分类问题 (Binary Classification)**:  是与非的判断，分类结果为两类，从中选择一个作为预测结果。例如：判断一封邮件是否为垃圾邮件 📧🚫。
- **多分类问题 (Multi-class Classification)**: 分类结果为多个类别，从中选择一个作为预测结果。例如：识别一张图片中的动物是猫 🐱、狗 🐶、鸟 🐦 还是兔子 🐰。
- **多标签分类问题 (Multi-label Classification)**: 不同于前两者，多标签分类问题一个样本的预测结果可能是多个，或者有多个标签。例如：一部电影可以同时被分为动作片 🎬 和犯罪片 🔪，一则新闻可以同时属于政治 🏛️ 和法律 ⚖️ 等。

## 分类分析常用算法 ⚙️

::: {.panel-tabset}
### 逻辑回归

-   **原理**: 特征和最终分类结果之间表示为线性关系，但是得到的结果是映射到整个实数域中的。对于分类问题，例如二分类问题需要将结果映射到{0,1}空间，因此仍需要一个函数g完成实数域到{0,1}空间的映射。逻辑回归中函数g则为Logistic函数，当g>0是，x的预测结果为正，反之为负。
-   **优点**: 直接对分类概率 (可能性) 进行建模，无需事先假设数据分布，是一个判别模型，并且Logistic函数是任意阶可导凸函数，许多数学方面的优化算法可以使用。
- **案例**：
    - **客户流失预测**：根据客户的历史行为数据（如消费金额、购买频率、活跃度等），预测客户是否会流失。
    - **信用风险评估**：根据借款人的个人信息、财务状况等，评估其违约风险。
    - **疾病诊断**：根据患者的症状、检查结果等，判断其是否患有某种疾病。

### 线性判别分析

-   **原理**: 针对训练集，将其投影到一条直线上，使得同类样本点尽量接近，异类样本点尽量远离。即同类样本计算得到的结果尽量比较相似，协方差较小，异类样本的中心间距离尽可能大，同时考虑两者可以得到线性判别分析的目标函数。
- **案例**：
    - **人脸识别**：将人脸图像投影到低维空间，使得同一人的图像尽可能接近，不同人的图像尽可能远离。
    - **手写数字识别**：将手写数字图像投影到低维空间，使得相同数字的图像尽可能接近，不同数字的图像尽可能远离。

### 支持向量机

-   **原理**: 基于训练集在样本空间中找到一个超平面可以将不同类别的样本分开，并且使得所有的点都尽可能的远离超平面。
-   **关键**: 如何找到一个最优的超平面以及最优超平面如何定义是支持向量机需要解决的问题。超平面应该对样本局部扰动的“容忍性”最好，即结果对于未知样本的预测更加准确。
- **案例**：
    - **文本分类**：将文本数据表示为向量，然后使用SVM进行分类，例如判断一封邮件是否为垃圾邮件。
    - **图像识别**：将图像数据表示为向量，然后使用SVM进行分类，例如识别图像中的物体。

### 决策树

-   **原理**: 可以完成对样本的分类，可以看作对于“当前样本是否属于正类”这一问题的决策过程，模仿人类做决策时的处理机制，基于树的结果进行决策。
-   **例子**: 信用卡申请评估。
- **案例**：
    - **客户细分**：根据客户的特征（如年龄、收入、职业等），将客户划分为不同的群体。
    - **医疗诊断**：根据患者的症状、检查结果等，判断其可能患有的疾病。

### K邻近

-   **原理**: 给定测试集合，基于某种距离度量计算训练集中与其最接近的k个训练样本，基于这个样本的信息对测试样本的类别进行预测。首先是k值的确定，距离计算公式的确定，以及k个样本对于测试样本的分类的影响的确定。
-   **特点**: 无需训练，训练时间开销为0，被称为“懒惰学习”。
- **案例**：
    - **推荐系统**：根据用户的历史行为（如购买记录、浏览记录等），找到与该用户相似的其他用户，然后将这些用户喜欢的商品推荐给该用户。
    - **异常检测**：找到与正常数据点距离较远的数据点，将其视为异常点。

### 朴素贝叶斯

-   **原理**: 基于贝叶斯定理，直接找出特征输出Y和特征X的联合分布P(X, Y)，然后用P(Y|X) = P(X, Y)/P(X)得出。是生成方法。
-   **贝叶斯定理公式**:

    $$
    P(B|A) = \frac{P(A|B)P(B)}{P(A)}
    $$
- **案例**：
    - **垃圾邮件过滤**：根据邮件中的关键词，判断邮件是否为垃圾邮件。
    - **情感分析**：根据文本中的词语，判断文本的情感倾向（如正面、负面）。
:::

## 分类分析——逻辑回归 (Logistic Regression) 🧠

### 概念解释 🤓

-   **线性关系**: 特征与分类结果之间的关系可以用线性方程表示，就像我们可以用一条直线 📏 来近似描述身高和体重的关系。
-   **实数域**: 所有实数的集合，就像一条无限延伸的数轴 ↔️。
-   **{0, 1} 空间**: 只有 0 和 1 两个值的集合，就像一个开关 💡，只有开和关两种状态。
-   **Logistic 函数**: 一种 S 形函数，可以将实数映射到 (0, 1) 区间，就像一个魔法 🧙‍♀️，将任何数字变成 0 到 1 之间的数字。
    -   公式：
    $$
        \sigma(z) = \frac{1}{1 + e^{-z}}
    $$

### 逻辑回归的优点 👍

-   直接对分类**概率**进行建模，无需事先假设数据分布，就像我们可以直接估计一件事情发生的可能性，而不需要知道事情发生的具体细节。
-   是一个**判别模型** (Discriminative Model)。
    -   **判别模型**: 直接学习预测模型，例如逻辑回归，就像我们直接学习如何区分猫 🐱 和狗 🐶，而不需要知道猫和狗是如何产生的。
    -   **生成模型** (Generative Model): 学习数据的联合分布，然后进行预测，例如朴素贝叶斯，就像我们先学习猫和狗的各种特征，然后根据这些特征来判断一个动物是猫还是狗。
-   Logistic 函数是**任意阶可导凸函数**，可以使用许多数学优化算法，就像我们可以用各种工具 🛠️ 来打磨一块玉石 💎。

## 分类分析——线性判别分析 (Linear Discriminant Analysis, LDA) 📐

### LDA 的核心思想 💡

- 将训练样本投影到一条直线上，使得：
    - **同类样本**的投影点尽可能**接近**，就像让好朋友们 🤝 站得近一些。
    - **异类样本**的投影点尽可能**远离**，就像让陌生人 🧍 保持距离。
    - 协方差尽可能小。

### 目标函数 🎯

LDA 的目标是最大化类间距离，最小化类内距离。可以通过数学公式推导出 LDA 的目标函数，就像我们可以用公式来计算投篮 🏀 的最佳角度。

## 分类分析——支持向量机 (Support Vector Machine, SVM) 🛡️

### SVM 的基本思想

- 基于训练集在样本空间中找到一个**超平面**可以将不同类别的样本分开,就像用一把刀 🔪 将苹果 🍎 和橙子 🍊 分开。
- 并且使得所有的数据点都尽可能的远离超平面
- 如何找到一个最优的超平面以及最优超平面如何定义是支持向量机需要解决的问题。
- 我们所需要寻找的超平面应该对样本局部扰动的“容忍性”最好,即结果对于未知样本的预测更加准确。

### 核心概念 🤓

-   **超平面 (Hyperplane)**: 在高维空间中，将数据划分为两部分的平面，就像在二维空间中，用一条直线将平面分成两部分。
    -   方程：$w \cdot x + b = 0$
    -   $w$: 法向量，决定超平面的方向，就像指南针 🧭 指引方向。
    -   $b$: 位移项，决定超平面与原点之间的距离，就像调整刀 🔪 的位置。
-   **函数间隔 (Functional Margin)**: $\gamma' = y(w \cdot x + b)$
    -   $y$: 样本的真实类别 (+1 或 -1)，就像标签 🏷️。
    -   $w \cdot x + b$: 样本点到超平面的“距离”，就像测量距离 📏。
    -   函数间隔的符号表示分类是否正确，大小表示确信度，就像考试分数 💯，越高越好。
-   **几何间隔 (Geometric Margin)**: $\gamma = \frac{y(w \cdot x + b)}{||w||_2}$
    -   几何间隔是点到超平面的真实距离，就像用尺子 📏 测量距离。
    -   $||w||_2$: 向量 $w$ 的 L2 范数（模长），就像计算向量的长度。
    - **支持向量 (Support Vectors)**: 距离超平面最近的几个训练样本点，就像支撑起整个分类的士兵 💂‍♀️💂。

### SVM 的优化目标 🎯

-   **目标**: 最大化几何间隔，就像让苹果和橙子之间的距离最大。
-   **约束**: 所有样本点都被正确分类，且位于支持向量定义的间隔之外，就像确保每个水果 🍏🍊 都被分到正确的类别。
-   **数学表达**:
$$
\max_{\mathbf{w}, b} \frac{2}{\|\mathbf{w}\|} \\
\text{s.t.} \quad y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1, \quad i=1,2,\ldots,m.
$$
-   **对偶问题**: 通过拉格朗日乘子法，可以将上述优化问题转化为对偶问题，更高效地求解，就像用更巧妙的方法解决问题。

### SVM 图示 🖼️

![SVM 图示](images/svm.png)

-   **实线**: 超平面，就像刀 🔪。
-   **虚线**: 间隔边界，就像安全线 🚧。
-   **圆圈和叉**: 不同类别的样本，就像苹果 🍎 和橙子 🍊。
-   **带箭头的虚线**: 支持向量到超平面的距离，就像测量距离 📏。

## 分类分析——决策树 (Decision Tree) 🌳

### 决策树的核心思想 💡

-   决策树是一种**树形结构**，用于模拟人类的决策过程，就像一棵有许多分叉的树 🌳。
-   通过一系列的**问题 (特征)**，对样本进行分类，就像玩“猜猜我是谁”的游戏 🕵️‍♀️。
-   **节点**:
    -   **内部节点**: 表示一个特征或属性，就像一个问题 🤔。
    -   **叶节点**: 表示一个类别，就像一个答案 ✅。
-   **分支**: 表示特征的取值，就像不同的选择 ➡️。

### 决策树示例：信用卡申请 💳

![决策树示例](images/decision_tree.png)

-   **问题**: 是否批准信用卡申请？
-   **特征**: 年龄、是否有房产、是否有固定工作、历史信用等级。
-   **决策过程**: 从根节点开始，根据样本的特征值，沿着相应的分支向下，直到到达叶节点，得到分类结果，就像沿着树枝 🌿 走到树叶 🍃。

## 分类分析——K 邻近 (K-Nearest Neighbors, KNN) 🏘️

### KNN 的工作机制 ⚙️

-   给定测试样本，计算它与训练集中每个样本的距离，就像测量你和每个邻居 🧍🧍‍♀️ 的距离。
-   找出距离最近的 *k* 个训练样本（*k* 个邻居），就像找到你最近的 *k* 个邻居。
-   根据这 *k* 个邻居的类别，预测测试样本的类别。
    -   **多数表决**: 选择 *k* 个邻居中出现次数最多的类别，就像投票 🗳️ 选出最受欢迎的邻居。
    -   **加权表决**: 根据距离的远近，对 *k* 个邻居的投票进行加权，就像给更近的邻居更多的投票权 ⚖️。

### KNN 的关键因素 🤔

-   **k 值的选择**:
    -   *k* 值过小，容易受噪声影响，就像只听一个邻居的意见，可能会被误导。
    -   *k* 值过大，容易受不相关样本影响，就像听太多人的意见，会变得犹豫不决。
-   **距离度量**:
    -   欧氏距离 (Euclidean Distance)，就像直线距离 📏。
    -   曼哈顿距离 (Manhattan Distance)，就像城市街区距离 🏙️。
    -   其他距离度量。

### KNN 的特点 👍

-   **懒惰学习 (Lazy Learning)**: 无需训练，直接使用训练集进行预测，就像“临时抱佛脚” 📖。
-   **急切学习 (Eager Learning)**: 需要在训练阶段对样本进行处理，就像“未雨绸缪” ☔️。

## 分类分析——朴素贝叶斯 (Naive Bayes) 🍀

### 贝叶斯定理 (Bayes' Theorem) 🤓

$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
$$

-   $P(B|A)$: 后验概率 (Posterior Probability)，就像在知道一些信息后，对事件发生的可能性的重新评估。
-   $P(A|B)$: 似然概率 (Likelihood)，就像事件发生后，观察到某些现象的可能性。
-   $P(B)$: 先验概率 (Prior Probability)，就像在没有任何信息的情况下，对事件发生的可能性的初步估计。
-   $P(A)$: 证据 (Evidence)，就像观察到的现象。

### 朴素贝叶斯分类器 🤖

-   **生成模型**: 学习特征 *X* 和类别 *Y* 的联合分布 *P(X, Y)*，就像先了解猫 🐱 和狗 🐶 的各种特征。
-   **预测**: 计算条件概率 *P(Y|X) = P(X, Y) / P(X)*，就像根据观察到的特征，判断是猫还是狗。
-   **朴素 (Naive)**: 假设特征之间相互独立，就像认为猫的颜色和体重之间没有关系，这在现实中可能不成立，但可以让计算更简单。

## 关联分析 (Association Analysis) 🔗

::: {.callout-note}
关联规则是描述数据库中数据项之间所存在的关系的规则，即根据一个事务中某些项的出现可导出另一些项在同一事务中也出现，即隐藏在数据间的关联或相互关系。

关联分析就像侦探 🕵️‍♀️ 在犯罪现场寻找线索，发现不同物品之间的关联。
:::

### 关联分析的应用 🛒

-   **购物篮分析**: 发现顾客购买商品之间的关联规则，例如著名的“啤酒与尿布” 🍺🧷 故事。
-   **其他应用**: 医疗诊断 🩺、网页浏览分析 🖱️、文本挖掘 📝 等。

### 关联规则的定义 🤓

-   **项集 (Itemset)**: 一组项目的集合，例如 {啤酒, 尿布}。
-   **事务 (Transaction)**: 一次购买记录，例如 {牛奶, 面包, 啤酒, 尿布}。
-   **关联规则 (Association Rule)**: 形如 X → Y 的蕴含式，其中 X 和 Y 是不相交的项集。
    -   例如: {啤酒} → {尿布}

### 关联规则的指标 📊

-   **支持度 (Support)**: 项集 X 和 Y 同时出现的概率。
    -   $Support(X \to Y) = P(X \cup Y)$
    -   就像计算同时购买啤酒和尿布的顾客比例。
-   **置信度 (Confidence)**: 在 X 出现的条件下，Y 出现的概率。
    -   $Confidence(X \to Y) = P(Y|X) = \frac{P(X \cup Y)}{P(X)}$
    -   就像计算购买啤酒的顾客中，也购买尿布的比例。
-   **例子**: 假设有 100 个顾客，其中：
    -   15 个顾客同时购买了啤酒和尿布。
    -   30 个顾客购买了啤酒。
    -   那么：
        -   Support({啤酒} → {尿布}) = 15/100 = 15%
        -   Confidence({啤酒} → {尿布}) = 15/30 = 50%

### 关联规则的指标 (续) 📈

-   **期望置信度 (Expected Confidence)**: Y 单独出现的概率。
    -   $Expected Confidence(X \to Y) = P(Y)$
    -   就像计算所有顾客中，购买尿布的比例。
-   **提升度 (Lift)**: 置信度与期望置信度的比值。
    -   $Lift(X \to Y) = \frac{Confidence(X \to Y)}{Expected Confidence(X \to Y)} = \frac{P(Y|X)}{P(Y)} = \frac{P(X \cup Y)}{P(X)P(Y)}$
    -   提升度反映了 X 的出现对 Y 的出现概率的影响程度，就像研究啤酒的出现是否会让尿布的销量增加。
    -   Lift > 1: 正相关，就像啤酒的出现会促进尿布的销量。
    -   Lift = 1: 不相关，就像啤酒的出现对尿布的销量没有影响。
    -   Lift < 1: 负相关，就像啤酒的出现会抑制尿布的销量。
-   **例子** (续): 假设有 100 个顾客，其中：
    -   25 个顾客购买了尿布。
    -   那么：
        -   Expected Confidence({啤酒} → {尿布}) = 25/100 = 25%
        -   Lift({啤酒} → {尿布}) = 50% / 25% = 2

### 关联规则挖掘定义 ⛏️

-   **目标**: 给定一个交易数据集 *T*，找出其中所有支持度 ≥ min_support 和置信度 ≥ min_confidence 的关联规则。
-   **步骤**:
    1.  **生成频繁项集 (Frequent Itemset Generation)**: 找出所有满足最小支持度的项集，就像找到所有经常被一起购买的商品组合。
    2.  **生成规则 (Rule Generation)**: 在上一步产生的频繁项集的基础上生成满足最小置信度的规则，就像从经常被一起购买的商品组合中，找到那些具有强关联关系的组合。

### 关联规则挖掘算法 ⚙️

::: {.panel-tabset}

#### Apriori 算法

-   **核心思想**: 基于两条定律，减少频繁项集的生成时间：
    1.  如果一个项集是频繁的，则它的所有子集都是频繁的。就像如果 {啤酒, 尿布, 牛奶} 经常被一起购买，那么 {啤酒, 尿布} 也一定经常被一起购买。
    2.  如果一个项集是非频繁的，则它的所有超集都是非频繁的。就像如果 {薯片} 很少被购买，那么 {薯片, 啤酒} 也一定很少被一起购买。
-   **例子**:
    -   如果 {A, B} 是频繁项集，则 {A} 和 {B} 一定是频繁项集。
    -   如果 {A} 是非频繁项集，则 {A, B}, {A, C}, {A, B, C} 等一定是非频繁项集。

#### FP-Tree 算法 🌳

-   **FP-Tree (Frequent Pattern Tree)**: 一种树形数据结构，用于存储频繁项集的信息，就像一棵“购物树” 🌳，树上的每个节点都代表一个商品，节点之间的路径代表商品之间的关联。
-   **构建 FP-Tree**:
    1.  扫描数据集，统计每个项的支持度，就像统计每个商品的销量。
    2.  根据支持度对项进行排序，就像将商品按照销量从高到低排序。
    3.  再次扫描数据集，将每个事务中的项按照排序后的顺序插入到 FP-Tree 中，就像将每个顾客的购物清单 🛒 按照商品销量排序后，添加到“购物树”中。
-   **挖掘 FP-Tree**: 从 FP-Tree 中递归地挖掘频繁项集，就像从“购物树”中找到那些经常出现的“树枝” 🌿。

##### 算法示例1

![FP-Tree 示例 1](images/fptree1.png)
原始数据中，一共有10条交易数据，分别统计每个商品的支持度计数，A出现了8次，记为`A:8`，其他商品同理。

##### 算法示例2

![FP-Tree 示例 2](images/fptree2.png)
设置支持度阈值为20%，因为一共有10条交易数据，所以支持度计数至少为2，所以将支持度计数小于2的商品删除。

##### 算法示例3

![FP-Tree 示例 3](images/fptree3.png)
将每条交易数据中的商品按照支持度技术排序。
比如第一条交易数据ABCEFO，按照新的支持度表排序为ACEBF。其他交易数据同理。

##### 算法示例4

![FP-Tree 示例 4](images/fptree4.png)
将排序好的交易数据添加到FP树中。
第一条数据ACEBF，则创建A:1, C:1, E:1, B:1, F:1的FP树分支。
第二条数据ACG，创建单独的A:1, C:1, G:1分支。
以此类推。

##### 算法示例5

![FP-Tree 示例 5](images/fptree5.png)
当插入第四条交易数据ACEGD时，发现可以与第二条数据ACG共享A:1, C:1的前缀，所以形成A:2, C:2, G:1, E:1, D:1的分支。
以此类推。

##### 算法示例6

![FP-Tree 示例 6](images/fptree6.png)

构建好的FP树。
挖掘FP-Tree：从FP-Tree中递归地挖掘频繁项集，比如以D为条件，找到D的条件模式基为<A:2, C:2>，这意味着在所有交易数据中，D和AC同时出现的次数为2次。其他商品同理。

#### PrefixSpan 算法

-   **PrefixSpan (Prefix-Projected Pattern Growth)**: 一种挖掘频繁序列的算法。
-   **序列 (Sequence)**: 一组有序的项集，例如 <(AB)(AC)D(CF)>，就像顾客按时间顺序购买的商品列表。
-   **子序列 (Subsequence)**: 如果序列 A 的所有项集都能在序列 B 的项集中找到，则 A 是 B 的子序列，就像顾客购买了商品列表 A 中的所有商品，那么 A 就是 B 的子序列。
-   **前缀 (Prefix)** 和 **后缀 (Suffix)**:
    -   例如，序列 <a(abc)(ac)d(cf)> 的前缀和后缀例子：

| 前缀      | 后缀 (前缀投影)       |
| ----------- | --------------------- |
| <\a>       | <(abc)(ac)d(cf)>   |
| <\aa>      | <(\_bc)(ac)d(cf)>  |
| <\ab>      | <(\_c)(ac)d(cf)>   |

##### PrefixSpan算法步骤

-   **输入**: 序列数据集S和支持度阈值α
-   **输出**: 所有满足支持度要求的频繁序列集
-   找出所有长度为1的前缀和对应的投影数据库
-   对长度为1的前缀进行计数,将支持度低于阈值α的前缀对应的项从数据集S删除,同时得到所有的频繁1项序列, i=1.
-   对于每个长度为i满足支持度要求的前缀进行递归挖掘:
    -   找出前缀所对应的投影数据库。如果投影数据库为空,则递归返回。
    -   统计对应投影数据库中各项的支持度计数。如果所有项的支持度计数都低于阈值, 则递归返回。
    -   将满足支持度计数的各个单项和当前的前缀进行合并, 得到若干新的前缀
    -   令i=i+1, 前缀为合并单项后的各个前缀, 分别递归执行第3步。

:::


## 聚类分析 (Cluster Analysis) 🤝

::: {.callout-note}
聚类分析是典型的无监督学习任务，训练样本的标签信息未知，通过对无标签样本的学习揭示数据内在性质及规律，这个规律通常是样本间相似性的规律。

聚类分析就像将不同的水果 🍎🍊🍌🍇 按照种类放在不同的篮子里 🧺。
:::

### 聚类分析的目标 🎯

-   将一组数据按照**相似性**和**差异性**分为几个类别。
-   使得：
    -   **同一类别**内的数据相似性**尽可能大**，就像让同一品种的水果 🍎🍎 放在一起。
    -   **不同类别**间的数据相似性**尽可能小**，就像让不同品种的水果 🍎🍊 分开。

### 聚类分析的应用 🏘️

-   **市场细分**: 将顾客划分为不同的群体，以便进行精准营销，就像将顾客按照年龄、收入等特征分成不同的群体。
-   **图像分割**: 将图像分割成不同的区域，以便进行目标识别，就像将照片中的天空 ☁️、树木 🌳、人物 🧍‍♀️ 分割开来。
-   **异常检测**: 发现数据中的异常点，例如信用卡欺诈 💳🚫，就像找出不正常的交易记录。

### 聚类分析的特点 🤔

-   **无监督学习**: 无需标签信息，就像在没有标签的情况下，将水果 🍎🍊🍌🍇 分类。
-   **探索性分析**: 发现数据中潜在的规律，就像探索未知的领域 🗺️。
-   **结果解释**: 需要对聚类结果进行解释，赋予其语义，就像给每个水果篮子 🧺 贴上标签 🏷️。

### 聚类分析算法 ⚙️

::: {.panel-tabset}
#### K-均值算法 (K-Means)

-   **原理**: 将样本划分到 *k* 个簇中，使得每个样本到其所属簇的中心的距离之和最小，就像将学生 🧑‍🎓 分到 *k* 个班级 🏫，使得每个学生到班级中心的距离之和最小。
-   **步骤**:
    1.  随机选取 *k* 个聚类中心，就像随机选择 *k* 个班长 🙋‍♀️🙋‍♂️。
    2.  重复以下过程，直到收敛：
        -   对于每个样本，计算其所属的类别（距离最近的中心），就像让每个学生选择离自己最近的班级。
        -   对于每个类别，重新计算聚类中心（该类别所有样本的均值），就像重新选举班长，选出班级的平均位置。
-   **缺点**:
    -   需要预先指定 *k* 值，就像需要预先确定班级的数量，这在实际中可能很难确定。
    -   对初始聚类中心敏感，就像不同的班长人选可能会导致不同的分班结果。
    -   对噪声和离群点敏感，就像个别学生的特殊情况可能会影响分班结果。
    -   可能陷入局部最优，就像分班结果可能不是最佳的。
- **公式**
    -   距离和E:
    $$
        E = \sum_{i=1}^{k}\sum_{x \in C_i}||x - \mu_i||_2^2
    $$
其中$\mu_i$是簇$C_i$的均值向量, 即：
$$
\mu_i = \frac{1}{|C_i|} \sum_{x \in C_i}x
$$

#### DBSCAN 算法

-   **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: 一种基于密度的聚类算法，就像根据人群的密度 🧑‍🤝‍🧑 来划分不同的区域。
-   **核心思想**: 将具有足够密度的区域划分为簇，并发现任意形状的簇，就像找出人群聚集的区域。
-   **关键概念**:
    -   **Eps 邻域**: 以点 *p* 为中心，半径为 Eps 的区域，就像以你为中心，画一个半径为 Eps 的圆 ⭕。
    -   **MinPts**: 给定的密度阈值，就像规定一个区域内至少有多少人 👨‍👩‍👧‍👦 才能算作人群聚集。
    -   **核心对象 (Core Object)**: Eps 邻域内包含至少 MinPts 个点的点，就像人群中的核心人物 😎。
    -   **直接密度可达 (Directly Density-Reachable)**: 如果点 *p* 在点 *q* 的 Eps 邻域内，且 *q* 是核心对象，则 *p* 从 *q* 直接密度可达，就像你和一个核心人物 😎 在同一个圈子里。
    -   **密度可达 (Density-Reachable)**: 如果存在一系列点，使得每个点从前一个点直接密度可达，则最后一个点从第一个点密度可达，就像你通过一系列朋友 🤝 认识了一个名人 🌟。
    -   **密度相连 (Density-Connected)**: 如果两个点都从同一个点密度可达，则这两个点密度相连，就像你和另一个人都通过朋友 🤝 认识了同一个名人 🌟。
-   **步骤**:
    1.  检查每个点的 Eps 邻域。
    2.  如果一个点的 Eps 邻域包含的点数多于 MinPts，则创建一个以该点为核心对象的新簇。
    3.  迭代地聚集从核心对象直接密度可达的对象。
    4.  当没有新的点添加到任何簇时，过程结束。

-   **优点**:
    -   无需预先指定簇的数量，就像不需要事先知道有多少个人群聚集区。
    -   可以发现任意形状的簇，就像可以找出各种形状的人群聚集区 🚶‍♀️🚶‍♂️🧍🧍‍♀️。
    -   对噪声不敏感，就像可以忽略那些离群的独行侠 👤。
-   **缺点**:
    -   对参数 Eps 和 MinPts 敏感，就像半径 Eps 和人数阈值 MinPts 的设置会影响聚类结果。
    -   当数据密度不均匀时，聚类效果较差，就像人群密度差异很大时，很难确定合适的半径和人数阈值。
    -   对于高维数据，存在“维度灾难” 😨，就像在高维空间中，很难定义“距离”和“密度”。

:::

## 回归分析 (Regression Analysis) 📈

::: {.callout-note}
回归分析方法反映的是事务数据库中属性值在时间上的特征,产生一个将数据项映射到一个实值预测变量的函数,发现变量或属性间的依赖关系,其主要研究问题包括数据序列的趋势特征、数据序列的预测以及数据间的相关关系等。

回归分析就像根据过去的天气数据 🌤️🌧️🌈 来预测明天的天气 ☔。
:::

### 回归分析的目标 🎯

-   研究变量之间的**相关关系**，就像研究身高和体重之间的关系。
-   建立**数学模型**进行预测，就像建立一个公式，根据身高来预测体重。

### 回归分析与分类 📊

-   **相似之处**: 都是监督学习问题，就像都需要有标签的数据 🏷️ 来进行学习。
-   **区别**:
    -   **分类**: 预测离散的类别标签，就像判断一张图片是猫 🐱 还是狗 🐶。
    -   **回归**: 预测连续的目标值，就像预测明天的气温 🌡️。

### 常见的回归分析模型 ⚙️

::: {.panel-tabset}
#### 线性回归分析 (Linear Regression)

-   **原理**: 用线性模型刻画特征向量 *X* 与回归目标 *y* 之间的关系，就像用一条直线 📏 来拟合身高和体重之间的关系。
-   **模型**:
    -   $f(x_i) = w_1x_{i1} + w_2x_{i2} + ... + w_nx_{in} + b$, 使得 $f(x_i) \approx y_i$
    -   就像找到一条直线，使得直线上的点尽可能接近真实的数据点。
-   **损失函数**:
    -   $L(w, b) = \sum_{i=1}^{m}(y_i - w^Tx_i - b)^2$
    -   就像计算每个数据点到直线的距离的平方和，然后最小化这个和。
-   **求解**: 最小二乘法 (Least Squares Method)，就像找到一条“最佳拟合”直线。
-   **广义线性模型 (Generalized Linear Model)**: 引入非线性函数 *g*，使得 *y* 和 *f(x)* 之间存在非线性关系，就像用曲线 〰️ 来拟合数据。
    -   $y_i = g^{-1}(w^Tx_i + b)$
    -   *g*: 联系函数 (Link Function)，就像一个“桥梁” 🌉，连接线性模型和非线性模型。

#### 支持向量回归 (Support Vector Regression, SVR) 🛡️

-   **原理**: 允许预测值 *f(x)* 和真实值 *y* 之间存在一定的偏差 (容忍度 *ε*)，就像允许预测的气温和真实气温之间存在一定的误差。
-   **核心思想**: 构建一个宽度为 2*ε* 的间隔带，落入间隔带内的样本被认为预测正确，就像在真实气温周围画一个“安全区” 🚧。

#### K 邻近回归 (K-Nearest Neighbors Regression) 🏘️

-   **原理**: 类似于 K 邻近分类，找到 *k* 个最近邻居，将这些邻居的回归目标的平均值作为预测值，就像根据你周围邻居的身高，来预测你的身高。
-   **加权平均**: 可以根据距离的远近，对邻居的回归目标进行加权平均，就像给更近的邻居更大的权重。
:::

## 总结 🎉

本章我们学习了数据分析和知识发现的几种常用方法，就像打开了一个工具箱 🧰，里面装满了各种神奇的工具：

-   **分类分析** 📦：将数据对象划分到不同的类别中，就像整理房间一样。
-   **关联分析** 🔗：发现数据项之间的关联规则，就像侦探 🕵️‍♀️ 发现线索。
-   **聚类分析** 🤝：将数据对象分成不同的簇，使得同一簇内的对象相似度高，不同簇之间的对象相似度低，就像朋友们聚会 🥳。
-   **回归分析** 📈：研究变量之间的关系，并建立模型进行预测，就像天气预报 🌤️。

这些方法在实际应用中非常广泛，可以帮助我们从数据中提取有价值的信息，并将其转化为知识，为决策提供支持。

## 思考与讨论 🤔

- 你之前是否接触过这些方法？在什么场景下接触到的呢？
- 你认为这些方法有哪些应用场景？你能举出一些具体的例子吗？
- 你认为这些方法的优缺点是什么？在实际应用中需要注意哪些问题？
- 你觉得还有什么需要补充的地方吗？
- 结合本章学习的内容，谈谈你在日常生活、学习和未来的工作中如何运用这些知识？
- 你在金融领域有自己感兴趣的方向吗？这些知识和技能对你有什么帮助？



